{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davinfalahtama/AI-Dikti/blob/main/playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocSKCXHI_UoL"
      },
      "outputs": [],
      "source": [
        "!pip install optimum\n",
        "!pip install accelerate\n",
        "!pip install auto-gptq\n",
        "!pip install PyPDF2\n",
        "!pip install langchain\n",
        "!pip install langchain_google_genai\n",
        "!pip install faiss-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45zFJX0OlvlK"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoConfig,AutoTokenizer\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()\n",
        "\n",
        "#Additional Info when using cuda\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.mem_get_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFbegAm9m92R",
        "outputId": "20856bf7-5315-4830-a91b-22e972607d58"
      },
      "outputs": [],
      "source": [
        "model_name= \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
        "\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "config.quantization_config[\"disable_exllama\"] = True\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             config=config,\n",
        "                                             revision=\"main\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_generation_pipeline = transformers.pipeline(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            task=\"text-generation\",\n",
        "            temperature=0.2,\n",
        "            max_new_tokens=50,\n",
        ")\n",
        "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEzDy3STNjl0",
        "outputId": "f7a39ed5-efe6-451e-b56f-5741c879e5ab"
      },
      "outputs": [],
      "source": [
        "prompt = \"Tell me about AI\"\n",
        "prompt_template=f'''<s>[INST] {prompt} [/INST]\n",
        "'''\n",
        "\n",
        "print(\"*** Pipeline:\")\n",
        "pipe = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    top_k=40,\n",
        "    repetition_penalty=1.1\n",
        ")\n",
        "\n",
        "print(pipe(prompt_template)[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Langchain Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import google.generativeai as genai\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "import pandas as pd\n",
        "from langchain_openai import OpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.runnables import ConfigurableFieldSpec\n",
        "from langchain_openai import ChatOpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Setup LLM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#OPEN-AI\n",
        "model =  ChatOpenAI(temperature=0.5,openai_api_key=\"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\")\n",
        "embedding = OpenAIEmbeddings(openai_api_key=\"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\")\n",
        "#GOOGLE-GEMINI\n",
        "# model = ChatGoogleGenerativeAI(model=\"gemini-pro\", \n",
        "#                                             temperature=0.5, \n",
        "#                                             convert_system_message_to_human=True,\n",
        "#                                             google_api_key=\"AIzaSyBYgcagyUPWzHFRyTZO3o8r85oZqmC25E8\")\n",
        "#embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key=\"AIzaSyBYgcagyUPWzHFRyTZO3o8r85oZqmC25E8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read File\n",
        "This code comprises two functions aimed at extracting text data from CSV and PDF files using Python libraries.\n",
        "\n",
        "**Used Libraries :**\n",
        "- Pandas : To extract text data from a CSV file.\n",
        "- PyPDF2.PdfReader : To extract text data from a PDF file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQ5xSjp8nFbD"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_csv(file):\n",
        "    \"\"\"\n",
        "    Function to extract text data from a CSV file.\n",
        "\n",
        "    Args:\n",
        "    - file (str): Path to the CSV file.\n",
        "\n",
        "    Returns:\n",
        "    - str: Concatenated text data from the specified column ('facts') in the CSV file.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(file)\n",
        "    return ' '.join(df['facts'])\n",
        "\n",
        "def extract_text_from_pdf(file):\n",
        "    \"\"\"\n",
        "    Function to extract text data from a PDF file.\n",
        "\n",
        "    Args:\n",
        "    - file (str): Path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "    - str: Extracted text data from all pages of the PDF file.\n",
        "    \"\"\"\n",
        "    pdf_text = \"\"\n",
        "    pdf_reader = PdfReader(file) \n",
        "    for page in pdf_reader.pages:\n",
        "        pdf_text += page.extract_text()\n",
        "    return pdf_text\n",
        "\n",
        "# Example usage:\n",
        "file = extract_text_from_pdf(\"docs/MTA023401.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split Text into Chunks\n",
        "\n",
        "- The **RecursiveCharacterTextSplitter** takes a large text and splits it based on a specified chunk size.\n",
        "- Chunking involves dividing the document into smaller, more manageable sections that fit comfortably within the context window of the large language model.\n",
        "\n",
        "More details can be found in the following link\n",
        "- [Understanding LangChain's RecursiveCharacterTextSplitter](https://dev.to/eteimz/understanding-langchains-recursivecharactertextsplitter-2846)\n",
        "- [Langchain Documentation](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e578frPzEDu6"
      },
      "outputs": [],
      "source": [
        "def split_text_into_chunks(text):\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "        return text_splitter.split_text(text)\n",
        "\n",
        "splited_text = split_text_into_chunks(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vector Store & Retriever\n",
        "- A vector store is a specialized database designed to store and manage vector embeddings.\n",
        "- A retriever is an interface that returns documents given an unstructured query\n",
        "- **FAISS** takes 2 mandatory parameters :\n",
        "   - *texts* : A list that contain string as elements\n",
        "   - *embedding* : Embedding models to transform all the text into embedding vectors\n",
        "\n",
        "More details can be found in the following link\n",
        "- [LangChain in Chains: Vector Stores](https://ai.plainenglish.io/langchain-in-chains-16-vector-stores-94ff578c1aee)\n",
        "- [LangChain in Chains: Retrievers](https://ai.plainenglish.io/langchain-in-chains-17-retrievers-1c252917f68f)\n",
        "- [Langchain Documentation](https://python.langchain.com/docs/integrations/vectorstores/faiss) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ua6XyJMZEqh0"
      },
      "outputs": [],
      "source": [
        "def create_vector_store(text_chunks):\n",
        "      vector_store = FAISS.from_texts(texts=text_chunks, embedding=embedding)\n",
        "      return vector_store\n",
        "\n",
        "vector_store = create_vector_store(splited_text)\n",
        "retriever = vector_store.as_retriever(search_type=\"similarity\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Contextualizing the question\n",
        "- Define a sub-chain that takes historical messages and the latest user question, and reformulates the question if it makes reference to any information in the historical information\n",
        "- **create_history_aware_retriever** create a chain that takes conversation history and returns documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "\n",
        "def contextualize_system_prompt():\n",
        "    contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
        "        which might reference context in the chat history, formulate a standalone question \\\n",
        "        which can be understood without the chat history. Do NOT answer the question, \\\n",
        "        If theres no chat history before then return it as it \\\n",
        "        just reformulate it if needed and otherwise return it as is.\"\"\"\n",
        "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", contextualize_q_system_prompt),\n",
        "            MessagesPlaceholder(\"chat_history\"),\n",
        "            (\"human\", \"{input}\"),\n",
        "        ]\n",
        "    )\n",
        "    history_aware_retriever = create_history_aware_retriever(model, retriever, contextualize_q_prompt)\n",
        "    return history_aware_retriever\n",
        "\n",
        "history_aware_retriever = contextualize_system_prompt()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chain with chat history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLJPK_HBFYo7"
      },
      "outputs": [],
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "def question_answer():\n",
        "    ### Answer question ###\n",
        "    prompt_template = \"\"\"\n",
        "                        You are a friendly chatbot named Bogu that helps to answer question regarding Indonesian Culture\\n\n",
        "                        Answer the question as detailed as possible from the provided context, make sure to provide all the details\\n\n",
        "                        If the question is about code, answer that you don't know the answer.\\n\n",
        "                        Context:\\n {context}?\\n\n",
        "                    \"\"\"\n",
        "    qa_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", prompt_template),\n",
        "            MessagesPlaceholder(\"chat_history\"),\n",
        "            (\"human\", \"{input}\"),\n",
        "        ]\n",
        "    )\n",
        "    question_answer_chain = create_stuff_documents_chain(model, qa_prompt)\n",
        "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
        "\n",
        "    return rag_chain\n",
        "\n",
        "conversational_rag_chain = question_answer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "### Statefully manage chat history ###\n",
        "store = {}\n",
        "\n",
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conversational_rag_chain = RunnableWithMessageHistory(\n",
        "        conversational_rag_chain,\n",
        "        get_session_history,\n",
        "        input_messages_key=\"input\",\n",
        "        history_messages_key=\"chat_history\",\n",
        "        output_messages_key=\"answer\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conversational_rag_chain.invoke(\n",
        "    {\"input\": \"Who are you\"},\n",
        "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conversational_rag_chain.invoke(\n",
        "    {\"input\": \"who create it\"},\n",
        "    config={\n",
        "        \"configurable\": {\"session_id\": \"abc123\"}\n",
        "    }, \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='Who are you'),\n",
              " AIMessage(content='I am Bogu, a friendly chatbot here to help answer questions about Indonesian Culture. Feel free to ask me anything related to Indonesian culture, traditions, or any other related topics!')]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = store.get('111')\n",
        "a.messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'HumanMessage' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m message \u001b[39min\u001b[39;00m a\u001b[39m.\u001b[39mmessages:\n\u001b[1;32m----> 2\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(message, HumanMessage):\n\u001b[0;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUser: \u001b[39m\u001b[39m{\u001b[39;00mmessage\u001b[39m.\u001b[39mcontent\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m   \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(message, AIMessage):\n",
            "\u001b[1;31mNameError\u001b[0m: name 'HumanMessage' is not defined"
          ]
        }
      ],
      "source": [
        "for message in a.messages:\n",
        "  if isinstance(message, HumanMessage):\n",
        "    print(f\"User: {message.content}\")\n",
        "  elif isinstance(message, AIMessage):\n",
        "    print(f\"Assistant: {message.content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from chat_models.openai import ChatOpenAi "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'111': ChatMessageHistory(messages=[HumanMessage(content='Who are you'), AIMessage(content='I am Bogu, a friendly chatbot here to help answer questions about Indonesian Culture. Feel free to ask me anything related to Indonesian culture, traditions, or any other related topics!')])}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "store = {}\n",
        "app = ChatOpenAi(store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_text = app.process_files(\"docs/MTA023401.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_chunks = app.split_text_into_chunks(raw_text)\n",
        "app.create_vector_store(text_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
              "| VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001D3801757C0>))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Given a chat history and the latest user question             which might reference context in the chat history, formulate a standalone question             which can be understood without the chat history. Do NOT answer the question,             If theres no chat history before then return it as it             just reformulate it if needed and otherwise return it as is.')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
              "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001D355DA9BE0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001D355DB9100>, temperature=0.5, openai_api_key=SecretStr('**********'), openai_proxy='')\n",
              "| StrOutputParser()\n",
              "| VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001D3801757C0>)), config={'run_name': 'chat_retriever_chain'})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "app.contextualize_system_prompt()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RunnableBinding(bound=RunnableAssign(mapper={\n",
              "  context: RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
              "           | VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001D3801757C0>))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Given a chat history and the latest user question             which might reference context in the chat history, formulate a standalone question             which can be understood without the chat history. Do NOT answer the question,             If theres no chat history before then return it as it             just reformulate it if needed and otherwise return it as is.')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
              "           | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001D355DA9BE0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001D355DB9100>, temperature=0.5, openai_api_key=SecretStr('**********'), openai_proxy='')\n",
              "           | StrOutputParser()\n",
              "           | VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001D3801757C0>)), config={'run_name': 'retrieve_documents'})\n",
              "})\n",
              "| RunnableAssign(mapper={\n",
              "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
              "              context: RunnableLambda(format_docs)\n",
              "            }), config={'run_name': 'format_inputs'})\n",
              "            | ChatPromptTemplate(input_variables=['chat_history', 'context', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"\\n                            You are a friendly chatbot named Bogu that helps to answer question regarding Indonesian Culture\\n\\n                            Answer the question as detailed as possible from the provided context, make sure to provide all the details\\n\\n                            If the question is about code, answer that you don't know the answer.\\n\\n                            Context:\\n {context}?\\n\\n                        \")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
              "            | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001D355DA9BE0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001D355DB9100>, temperature=0.5, openai_api_key=SecretStr('**********'), openai_proxy='')\n",
              "            | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
              "  }), config={'run_name': 'retrieval_chain'})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "app.get_conversational_chain()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input': 'Who are you',\n",
              " 'chat_history': [],\n",
              " 'context': [Document(page_content='b. Pengumpulan Data Sekunder \\nData sekunder berupa literature /teori-teori dan informasi-\\ninformasi yang berkaitan dengan tujuan penelitian, dan akan \\ndikaitkan dengan hasil dari lapangan dengan tujuan untuk \\nmenganalisis data. Adapun literature  yang di peroleh dalam \\npenelitian ini berupa buku-buku, jurnal, tesis, disertasi, peraturan \\npemerintah, dan website .  \\n \\n3. Tahap Pembahasan \\nDalam menganalisis data maka peneliti harus menyesuaikan \\ndata yang didapatkan dengan permasalahan dan tujuan dari penelitian \\nagar kemudian dibahas. Metode analisis merupakan salah satu cara \\nuntuk mencapai tujuan dari penelitian. Metode analisis data menurut \\nPaton dalam (Moleong L. J., 2007) adalah proses mengatur urutan \\ndata, mengorganisasikan nya dalam suatu pola, kategori, dan satuan \\nuraian dasar.  \\nPendekatan strukturalisme dan penelusuran sejarah dalam \\nmelakukan proses pembahasan dapat diungkapkan dengan beberapa \\nlangkah-langkah yang akan dicapai. Langkah-langkah ini berpedoman  \\n 14'),\n",
              "  Document(page_content='13 \\n \\npeneliti mengamati pengguna ruang yaitu pengunjung dan \\npenyedia jasa kemudian akan dikaitkan dengan bentuk dan \\nfungsi yang akan menghasilkan sebuah makna.  \\n \\nb. Pengumpulan Data Sekunder \\nData sekunder berupa literature /teori-teori dan informasi-\\ninformasi yang berkaitan dengan tujuan penelitian, dan akan \\ndikaitkan dengan hasil dari lapangan dengan tujuan untuk \\nmenganalisis data. Adapun literature  yang di peroleh dalam \\npenelitian ini berupa buku-buku, jurnal, tesis, disertasi, peraturan \\npemerintah, dan website .  \\n \\n3. Tahap Pembahasan \\nDalam menganalisis data maka peneliti harus menyesuaikan \\ndata yang didapatkan dengan permasalahan dan tujuan dari penelitian \\nagar kemudian dibahas. Metode analisis merupakan salah satu cara \\nuntuk mencapai tujuan dari penelitian. Metode analisis data menurut \\nPaton dalam (Moleong L. J., 2007) adalah proses mengatur urutan \\ndata, mengorganisasikan nya dalam suatu pola, kategori, dan satuan \\nuraian dasar.'),\n",
              "  Document(page_content='terhadap perilaku penggunanya? \\n1.3. TUJUAN PENELITIAN  \\n \\nMengetahui pengaruh sejarah perkembangan Tugu Yogyakarta dan \\nkawasan di sekitar terhadap maknanya. \\nMengetahui pengaruh makna Tugu Yogyakarta dan kawasan di sekitarnya \\nterhadap perilaku penggunanya. \\n  \\n 10 \\n \\n1.4. METODE PENELITIAN \\n \\n1.4.1.  Jenis Penelitian  \\n \\nPenelitian yang dilakukan di Tugu Yogyakarta dan kawasan sekitarnya \\nmerupakan penelitian yang bertujuan ingin membaca dan menemukan pengaruh \\nmakna terhadap perilaku pengguna ruang di kawasan Tugu Yogyakarta. \\nMetodologi dalam penelitian ini juga mengacu pada tujuan penelitian yang telah \\nditentukan yaitu untuk mengetahui sejarah perkembangan makna dari Tugu \\nYogyakarta dan kawasan sekitar, dan makna Tugu Yogyakarta dan kawasan \\nsekitar saat ini.  \\nSesuai dengan tujuan penelitian yaitu untuk mengetahui pengaruh sejarah \\nperkembangan Tugu Yogyakarta dan kawasan di sekitar terhadap maknanya, dan \\nmengetahui pengaruh makna Tugu Yogyakarta dan kawasan di sekitarnya'),\n",
              "  Document(page_content='belakangnya adalah Tugu kemudian ada pula yang berfoto-foto dekat di kaki \\nTugu. Perilaku pengunjung yang lain adalah duduk-duduk di trotoar, bangku yang \\nsudah disediakan, dan lesehan  di area diorama sambil makan dan minum. Selain \\nitu perilaku pengunjung ada pula yang hanya berlalu lalang atau sekedar lewat dan \\nberhenti sejenak untuk berfoto dan kemudian melakukan perjalanan lagi.  \\nPerilaku penyedia jasa seperti berjualan sangat beraneka ragam, yaitu \\npedagang angkringan  sebagai salah satu makanan khas Yogyakarta, pedagang \\nlalapan , pedagang nasi goreng, pedagang asongan, dan wedang ronde . Selain itu \\nperilaku penyedia jasa seperti pengamen biasanya melihat pengunjung sebagai \\nkonsumennya, pengunjung sebagai target biasanya adalah pengunjung yang \\nsedang duduk-duduk dan makan.  Perilaku penyedia jasa yang lain adalah pentas  \\n 8 \\n \\nseni seperti penyedia jasa melakukan dandanan dan menggunakan kostum seperti')],\n",
              " 'answer': 'I am Bogu, a friendly chatbot here to help answer questions about Indonesian Culture. Feel free to ask me anything related to Indonesian culture, traditions, or any other related topics!'}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "app.run_invoke(\"Who are you\",\"111\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "app.store"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
