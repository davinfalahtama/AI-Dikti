{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davinfalahtama/AI-Dikti/blob/main/playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optimum\n",
        "!pip install accelerate\n",
        "!pip install auto-gptq\n",
        "!pip install PyPDF2\n",
        "!pip install langchain\n",
        "!pip install langchain_google_genai\n",
        "!pip install faiss-gpu"
      ],
      "metadata": {
        "id": "ocSKCXHI_UoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "45zFJX0OlvlK"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoConfig,AutoTokenizer\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import google.generativeai as genai\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SY5PWMYjlvlM"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name= \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
        "\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "config.quantization_config[\"disable_exllama\"] = True\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             config=config,\n",
        "                                             revision=\"main\").to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "text_generation_pipeline = transformers.pipeline(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            task=\"text-generation\",\n",
        "            temperature=0.2,\n",
        "            repetition_penalty=1.1,\n",
        "            return_full_text=True,\n",
        "            max_new_tokens=300,\n",
        ")\n",
        "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
      ],
      "metadata": {
        "id": "jFbegAm9m92R",
        "outputId": "20856bf7-5315-4830-a91b-22e972607d58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n",
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4193: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Tell me about AI\"\n",
        "prompt_template=f'''<s>[INST] {prompt} [/INST]\n",
        "'''\n",
        "\n",
        "print(\"*** Pipeline:\")\n",
        "pipe = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    top_k=40,\n",
        "    repetition_penalty=1.1\n",
        ")\n",
        "\n",
        "print(pipe(prompt_template)[0]['generated_text'])"
      ],
      "metadata": {
        "id": "vEzDy3STNjl0",
        "outputId": "f7a39ed5-efe6-451e-b56f-5741c879e5ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Pipeline:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(file):\n",
        "      pdf_text = \"\"\n",
        "      pdf_reader = PdfReader(file)\n",
        "      for page in pdf_reader.pages:\n",
        "          pdf_text += page.extract_text()\n",
        "      return pdf_text\n",
        "\n",
        "file = extract_text_from_pdf(\"MTA023401.pdf\")"
      ],
      "metadata": {
        "id": "cQ5xSjp8nFbD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text_into_chunks(text):\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
        "        return text_splitter.split_text(text)\n",
        "\n",
        "splited_text = split_text_into_chunks(file)"
      ],
      "metadata": {
        "id": "e578frPzEDu6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vector_store(text_chunks):\n",
        "      embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key=\"AIzaSyBYgcagyUPWzHFRyTZO3o8r85oZqmC25E8\")\n",
        "      vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)\n",
        "      return vector_store\n",
        "\n",
        "vector_store = create_vector_store(splited_text)"
      ],
      "metadata": {
        "id": "ua6XyJMZEqh0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "def get_conversational_chain(llm,vector_chain):\n",
        "    prompt_template = \"\"\"\n",
        "        ### [INST]\n",
        "        Instruction: Answer the question based on your knowledge. Here is context to help:\n",
        "\n",
        "        {context}\n",
        "\n",
        "        ### QUESTION:\n",
        "        {question}\n",
        "\n",
        "        [/INST]\n",
        "    \"\"\"\n",
        "    retriever = vector_chain.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
        "\n",
        "\n",
        "    custom_rag_prompt = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "    rag_chain = (\n",
        "      {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "      | custom_rag_prompt\n",
        "      | llm\n",
        "      | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    return rag_chain\n",
        "\n",
        "\n",
        "rag_chain =  get_conversational_chain(mistral_llm,vector_store)"
      ],
      "metadata": {
        "id": "rLJPK_HBFYo7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain.invoke(\"What is Task Decomposition?\")"
      ],
      "metadata": {
        "id": "THMdXlfOG9dn",
        "outputId": "3ec0e03e-3971-4023-ea5b-832e7af92360",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K0-qxM2dIezr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}