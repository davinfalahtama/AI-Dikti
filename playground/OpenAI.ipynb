{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Models for OPEN-AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Environtment API key\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "#OPENAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "#Read File\n",
    "from PyPDF2 import PdfReader\n",
    "import pandas as pd\n",
    "\n",
    "#Split Text to Chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "#Vector Store & Retriever\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "#Contextualing Question\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model & Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "LLM =  ChatOpenAI(temperature=0.5, openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "EMBEDDING = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read File\n",
    "This code comprises two functions aimed at extracting text data from CSV and PDF files using Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_csv(file):\n",
    "    \"\"\"\n",
    "    Function to extract text data from a CSV file.\n",
    "\n",
    "    Args:\n",
    "    - file (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - str: Concatenated text data from the specified column ('facts') in the CSV file.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file)\n",
    "    return ' '.join(df['facts'])\n",
    "\n",
    "def extract_text_from_pdf(file):\n",
    "    \"\"\"\n",
    "    Function to extract text data from a PDF file.\n",
    "\n",
    "    Args:\n",
    "    - file (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    - str: Extracted text data from all pages of the PDF file.\n",
    "    \"\"\"\n",
    "    pdf_text = \"\"\n",
    "    pdf_reader = PdfReader(file) \n",
    "    for page in pdf_reader.pages:\n",
    "        pdf_text += page.extract_text()\n",
    "    return pdf_text\n",
    "\n",
    "# Example usage:\n",
    "file = extract_text_from_pdf(\"../docs/MTA023401.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Text into Chunks\n",
    "\n",
    "- The **RecursiveCharacterTextSplitter** takes a large text and splits it based on a specified chunk size.\n",
    "- Chunking involves dividing the document into smaller, more manageable sections that fit comfortably within the context window of the large language model.\n",
    "\n",
    "More details can be found in the following link\n",
    "- [Understanding LangChain's RecursiveCharacterTextSplitter](https://dev.to/eteimz/understanding-langchains-recursivecharactertextsplitter-2846)\n",
    "- [Langchain Documentation](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    return text_splitter.split_text(text)\n",
    "\n",
    "splited_text = split_text_into_chunks(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Store & Retriever\n",
    "\n",
    "- A vector store is a specialized database designed to store and manage vector embeddings.\n",
    "- A retriever is an interface that returns documents given an unstructured query\n",
    "- **FAISS** takes 2 mandatory parameters :\n",
    "   - *texts* : A list that contain string as elements\n",
    "   - *embedding* : Embedding models to transform all the text into embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(text_chunks):\n",
    "      vector_store = FAISS.from_texts(texts=text_chunks, embedding=EMBEDDING)\n",
    "      return vector_store\n",
    "\n",
    "vector_store = create_vector_store(splited_text)\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextualizing the question\n",
    "\n",
    "- Define a sub-chain that takes historical messages and the latest user question, and reformulates the question if it makes reference to any information in the historical information\n",
    "- **create_history_aware_retriever** create a chain that takes conversation history and returns documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contextualize_system_prompt():\n",
    "    contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "        which might reference context in the chat history, formulate a standalone question \\\n",
    "        which can be understood without the chat history. Do NOT answer the question, \\\n",
    "        If theres no chat history before then return it as it \\\n",
    "        just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", contextualize_q_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    history_aware_retriever = create_history_aware_retriever(LLM, retriever, contextualize_q_prompt)\n",
    "    return history_aware_retriever\n",
    "\n",
    "history_aware_retriever = contextualize_system_prompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain with History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
